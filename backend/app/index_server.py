import os
import pickle
# from openai import AzureOpenAI
from dotenv import load_dotenv
import openai
from multiprocessing import Lock
from multiprocessing.managers import BaseManager

from llama_index.core import (
    SimpleDirectoryReader, 
    VectorStoreIndex, 
    StorageContext, 
    load_index_from_storage,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding

from typing import Optional, Any, Dict

# Settings.embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-mpnet-base-v2", device="cpu")
# Settings.llm = Ollama(model="gemma2:27b", temperature=0, request_timeout=500.0, device="cpu")

load_dotenv()

index: Optional[Any] = None
stored_docs: Optional[Any] = {}
lock = Lock()

index_name = "./saved_index"
pkl_name = "stored_documents.pkl"

def initialize_index():
    """
    The `initialize_index` function creates a new global index or loads one from a pre-set path,
    utilizing sentence splitting and embedding models.
    """
    global index, stored_docs
    
    transformations = SentenceSplitter(chunk_size=512)
    # embed_model = OpenAIEmbedding(model_name="text-embedding-3-small")
    embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-mpnet-base-v2", device="cpu")
    ## Usando o Azure
    # embed_model = AzureOpenAIEmbedding(
    #     model="text-embedding-ada-002",
    #     deployment_name="my-custom-embedding",
    #     api_key=api_key,
    #     azure_endpoint=azure_endpoint,
    #     api_version=api_version,
    # )
    with lock:
        if os.path.exists(index_name):
            index = load_index_from_storage(
                StorageContext.from_defaults(persist_dir=index_name), embed_model=embed_model
            )
        else:
            index = VectorStoreIndex(nodes=[], embed_model=embed_model)
            index.storage_context.persist(persist_dir=index_name)
        if os.path.exists(pkl_name):
            with open(pkl_name, "rb") as f:
                stored_docs = pickle.load(f)


def query_index(query_text):
    """
    The function `query_index` queries the global index using a language model to retrieve relevant
    information based on the input query text.
    
    :param query_text: The `query_index` function takes a `query_text` parameter, which is the text used
    to query the global index. The function then uses a language model (llm) to query the index and
    returns the response. The parameters used for querying the index include `similarity_top_k` and
    :return: The function `query_index(query_text)` returns the response obtained by querying the global
    index using the provided `query_text`. The response is generated by utilizing a language model (llm)
    and the query engine with specified parameters such as similarity_top_k.
    """
    global index
    # llm = OpenAI(model="gpt-4o-mini")
    # llm = Ollama(model="llama3.2:1b", temperature=0, device="cpu")
    # response = index.as_query_engine(
    #     similarity_top_k=2,
    #     llm=llm,
    # ).query(query_text)

    llm = AzureOpenAI(
        model="gpt-4o-mini",
        deployment_name="gpt-4o-mini",
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_version="2024-02-01",
    )
    
    query_engine = index.as_query_engine(
        similarity_top_k=2,
        llm=llm,
    )
    response = query_engine.query(query_text)
    
    return response


def insert_into_index(doc_file_path, doc_id=None):
    """
    The function `insert_into_index` inserts a new document into a global index, storing the document's
    text and ID in a dictionary and persisting the index to a directory.
    
    :param doc_file_path: The `doc_file_path` parameter in the `insert_into_index` function is the file
    path of the document that you want to insert into the global index
    :param doc_id: The `doc_id` parameter in the `insert_into_index` function is used to specify the
    unique identifier for the document being inserted into the global index. If a `doc_id` is provided,
    it will be assigned to the document being inserted. If `doc_id` is `None`, the
    :return: The function `insert_into_index` does not explicitly return any value. It simply performs
    operations to insert a new document into the global index and update the stored documents
    dictionary.
    """
    global index, stored_docs
    documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()

    with lock:
        for document in documents:
            if doc_id is not None:
                document.id_ = doc_id
            index.insert(document)

            stored_docs[document.id_] = document.text[0:200]  # only take the first 200 chars

        index.storage_context.persist(persist_dir=index_name)

        first_document = documents[0]
        # Keep track of stored docs -- llama_index doesn't make this easy
        stored_docs[first_document.doc_id] = first_document.text[0:200] # only take the first 200 chars

        with open(pkl_name, "wb") as f:
            pickle.dump(stored_docs, f)

    return


def get_documents_list():
    """
    The function `get_documents_list` retrieves the list of currently stored documents along with their
    IDs and text.
    :return: The function `get_documents_list` returns a list of dictionaries, where each dictionary
    contains the id and text of a document stored in the `stored_docs` dictionary.
    """
    global stored_doc
    documents_list = []
    for doc_id, doc_text in stored_docs.items():
        documents_list.append({"id": doc_id, "text": doc_text})

    return documents_list


def delete_document_from_index():
    print("deletou")


import socket, subprocess, sys    
def is_port_in_use(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(('127.0.0.1', port)) == 0

# COMANDOS DO WINDOWS
def get_pid_using_port(port):
    result = subprocess.run(['netstat', '-ano'], capture_output=True, text=True)
    for line in result.stdout.splitlines():
        if f':{port} ' in line:
            return int(line.split()[-1])
    return None

import time
from tqdm import tqdm

if __name__ == "__main__":
    # init the global index
    print("initializing index...")
    initialize_index()

    # Simular progresso da inicialização do servidor
    for _ in tqdm(range(100), desc="Iniciando servidor", ncols=100):
        time.sleep(0.05)  # Simula a inicialização
        
    # setup server
    # NOTE: you might want to handle the password in a less hardcoded way
    port = 5002
    if is_port_in_use(port):
        pid = get_pid_using_port(port)
        if pid:
            print(f"Porta {port} já está em uso pelo processo {pid}. Terminando o processo.")
            os.system(f"taskkill /PID {pid} /F")
        else:
            print(f"Porta {port} está em uso, mas não foi possível encontrar o PID.")
            
    manager = BaseManager(address=('127.0.0.1', port), authkey=b'password')
    manager.register('query_index', query_index)
    manager.register('insert_into_index', insert_into_index)
    manager.register('get_documents_list', get_documents_list)
    server = manager.get_server()

    print("index server started...")
    server.serve_forever()

